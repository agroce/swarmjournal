%% -*- mode: LaTeX -*-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper focuses on answering a single question: In random testing,
can a diverse set of \emph{testing configurations} perform better than
a single, possibly ``optimal'' configuration? 
%
An example of a test configuration would be, for example, a list of
API calls that can be included in test cases.
%
Conventional wisdom in random testing~\cite{Hamlet94} has assumed a
policy of finding a ``good'' configuration and running as many tests
as possible with that configuration.
%
Considerable research effort has been devoted to the question of how
to tune a ``good configuration,'' e.g., how to use genetic algorithms
to optimize the \emph{frequency} of various method
calls~\cite{AndrewsL07}, or how to choose a length for
tests~\cite{ASE08}.
%
As a rule, the notion that some test configurations are ``good'' and
that finding a good (if not truly optimal, given the size of the
search space) configuration is important has not been challenged.
Furthermore, in the interests of maximizing coverage and fault
detection, it has been assumed that a good random test configuration
includes as many API calls or other input domain features as possible,
and this has been the guiding principle in large-scale efforts to test
C compilers~\cite{csmith}, file systems~\cite{ICSEDiff}, and utility
libraries~\cite{Pacheco}.  The rare exceptions to this rule have been
cases where a feature makes tests too difficult to evaluate or slow to
execute, or when static analysis or hand inspection can demonstrate
that an API call is unrelated to state~\cite{ICSEDiff}.  For example,
including pointer assertions may make compiling random C programs too
slow with some compilers.

In general, if a call or feature is omitted from some tests, it is
usually omitted from all tests.  This approach seems to make intuitive
sense: omitting features, unless it is necessary, means \emph{giving
up on detecting some faults}.  However, this objection to feature
omission only holds so long as testing is performed using a single
test configuration.  Swarm testing, in contrast, uses a diverse
``swarm'' of test configurations, each of which \emph{deliberately
omits certain API calls or input features}.
%
As a result, given a fixed testing budget, swarm testing tends to test
a more diverse set of inputs than would be tested under a so-called
``optimal'' configuration (perhaps better referred to as a
\emph{default} configuration) in which every feature is available for
use by every test.

\begin{figure}
\centering
\subfigure[Random testing with uniform probabilities]{%
\ifpdf% to omit figures from HTML
\includegraphics[width=0.90\columnwidth]{stack_example/no_swarm-crop}%
\fi%
\label{fig:stack:noswarm}}
\subfigure[Swarm testing]{%
\ifpdf%to omit figures from HTML
\includegraphics[width=0.90\columnwidth]{stack_example/swarm-crop}%
\fi%
\label{fig:stack:swarm}} \caption{Swarm testing changes the
  distribution of test cases for a stack. If push and pop operations
  are selected with equal probability, about 1~in 370,000 test cases
  will trigger a bug in a 32-element stack's overflow-detection logic.
  Swarm testing (note the test cases concentrated near the x-axis and
  y-axis of~\autoref{fig:stack:swarm}) triggers this bug in about 1~of
  every 16~cases.}
\label{fig:stack}
\end{figure}

One can visualize the impact of swarm testing by imagining a ``test
space'' defined by the contents of tests. As a simple example,
consider testing an implementation of a stack ADT that provides two
operations, push and pop. One can visualize the test space for the
stack ADT using these features as axes: each test is
characterized by the number of times it invokes each operation.
%
Any method for randomly generating test cases results in a probability
distribution over the test space, with the value at each point $(x,y)$ giving
the probability that a given test will contain exactly $x$ pushes and $y$ pops
(in any order).
%
To make this example more interesting, imagine the stack implementation has a
capacity bug, and will crash whenever the stack is required to hold
more than 32~items.


\autoref{fig:stack:noswarm} illustrates the situation for testing the stack
with a test generator that chooses pushes and pops with equal probability.  The
generator randomly chooses an input length
% (according to a gaussian distribution)
and then decides if each operation is a push or a pop.  The graph shows the
distribution of tests produced by this generator over the test space.  The
graph also shows contour lines for significant regions of the test space.
Where $P_{fail}=1$, a test chosen randomly from that region is certain to
trigger the stack's capacity bug; where $P_{fail}=0$, no test can trigger the
bug.
%
As \autoref{fig:stack:noswarm} shows, this generator only rarely produces
test cases that can trigger the bug.


Now consider a test generator based on swarm testing. This generator
first chooses a non-empty subset of the stack API and then generates a
test case using that subset. 
%
Thus, one-third of the test cases contain both pushes and pops,
one-third just pushes, and one-third just pops.
%
\autoref{fig:stack:swarm} shows the distribution of test cases output by
this generator.  As is evident from the graph, this generator often produces
test cases that trigger the capacity bug.


Although simple, this example illustrates the dynamics that make
swarm testing work.
%
The low dimensionality of the stack example is contrived, of course, and we
certainly believe that programmers should make explicit efforts to test
boundary conditions.
%
As evidenced by the results presented in this paper, however, swarm testing
generalizes to real situations in which there may be dozens of features that
can be independently turned on or off.  It also generalizes to testing real
software in which faults are very well hidden.

Every test generated by any swarm configuration can, in principle, be
generated by a test configuration with all features enabled.
%
However---as the stack example illustrates---the probability of
covering parts of the state space and detecting certain faults can be
demonstrably higher when a diverse set of configurations is tested.


Swarm testing has several important advantages.
%
First, it is low cost: in our experience, existing random test case
generators already support or can be easily adapted to support feature omission.
%
Second, swarm testing reduces the amount of human effort that must be devoted
to tuning the random tester.
%
In our experience, tuning is a significant ongoing burden.
%
Finally---and most importantly---swarm testing makes significantly
better use of a fixed CPU time budget than does random testing using a
single test configuration, in terms of both coverage and fault
detection.
%
For example, we performed an experiment where two machines, differing
only in that one used swarm testing and one did not, used
Csmith~\cite{csmith} to generate tests for a collection
of production-quality C compiler versions for x86-64.
% 
During one week of testing, the swarm machine found 104 distinct ways
to crash compilers in the test suite whereas the other
machine---running the default Csmith test configuration, which enables all
features---found only 73.
%
An improvement of more than 40\% in terms of number of bugs found,
using a random tester that has been intensively tuned for several
years, is surprising and significant.


Even more surprising were some of the details.
%
We found, for example, a compiler bug that could only be triggered by
programs containing pointers, but which was almost never triggered by
inputs that contained arrays.
%
This is odd because pointer dereferences and array accesses are very
nearly the same thing in C\@.\footnote{In C/C++, \texttt{a[i]} is
  syntactic sugar for \texttt{*(a+i)}.}
% 
Moreover, we found another bug in the same compiler that was only
triggered by programs containing arrays, but which was almost never
triggered by inputs containing pointers.
%
Fundamentally, it appears that omitting features while
generating random test cases can lead to improved test effectiveness.


Our contributions are as follows.
%
First, we characterize \emph{swarm testing}, a pragmatic variant of
random testing that increases the diversity of generated test
cases with little implementation effort.
%
The swarm approach to diversity differs from previous methods
in that it focuses solely on \emph{feature omission diversity}:
variance in which possible input features are \emph{not} present in
test cases.
%
Second, we show that---in three case studies---swarm testing offers
improved coverage and bug-finding power.
%
Third, we offer some explanations as to \emph{why} swarm testing works.
